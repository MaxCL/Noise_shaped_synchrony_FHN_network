{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Table 1 — Clustering Scores for different k-means values.\n",
        "\n",
        "This notebook reproduces **Table 1** of the manuscript  \n",
        "*\"Noise-shaped Synchrony in Neuronal Oscillator Networks\"*.\n",
        "\n",
        "It computes and reports **clustering and classification scores** obtained from the analysis of precomputed network synchrony data.\n",
        "\n",
        "---\n",
        "\n",
        "## What this notebook does\n",
        "\n",
        "- Loads precomputed feature and label data from CSV files  \n",
        "- Applies k-means clustering and related classification metrics  \n",
        "- Computes quantitative clustering scores reported in the manuscript\n",
        "\n",
        "This notebook **does not run full network simulations** and is therefore safe and fast to execute.\n",
        "\n",
        "---\n",
        "\n",
        "## Input data\n",
        "\n",
        "The following data files (located in this folder) are used as inputs:\n",
        "\n",
        "- Precomputed feature matrices and label CSV files used for clustering analysis\n",
        "\n",
        "---\n",
        "\n",
        "## Output\n",
        "\n",
        "Running all cells will generate the numerical results reported in **Table 1** of the manuscript and any supporting diagnostic plots saved by the notebook.\n",
        "\n",
        "---\n",
        "\n",
        "## How to run\n",
        "\n",
        "Simply execute all cells from top to bottom:\n",
        "\n",
        "- Jupyter menu: **Kernel → Restart & Run All**\n",
        "- Or execute cells sequentially\n",
        "\n",
        "Typical runtime on a standard laptop: **a few seconds to under one minute**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ef508163-6af9-4385-b874-4335cb8a2603",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        "    davies_bouldin_score,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "218d2837-aa85-4efc-ba60-db2b501a8690",
      "metadata": {},
      "outputs": [],
      "source": [
        "noise_coupling_file = \"n_c_.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "931e36d2-d994-419d-b4fa-1761b2b663a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def run_kmeans_on_file(input_file, ks=(3,4,5,6,7), k_for_labels=6, random_state=0, n_init=10, algorithm=\"lloyd\"):\n",
        "    import pandas as pd\n",
        "    from sklearn.cluster import KMeans\n",
        "    from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "    import numpy as np\n",
        "\n",
        "    noise_coupling_file = \"n_c_.csv\"\n",
        "\n",
        "    # Derive base name the same way as before\n",
        "    base_parts = input_file.split(\"_\")\n",
        "    print(base_parts)\n",
        "    base = f\"{base_parts[1]}_{base_parts[3]}\" if len(base_parts) >= 4 else base_parts[0]\n",
        "\n",
        "    # Read data\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    # --- Build the noise/coupling frame robustly ---\n",
        "    nc = None\n",
        "    try:\n",
        "        nc = pd.read_csv(noise_coupling_file, header=None, names=[\"noise\", \"coupling\"])\n",
        "    except Exception:\n",
        "        nc = None\n",
        "\n",
        "    # Prefer columns from df if present\n",
        "    if {\"noise\",\"coupling\"}.issubset(df.columns):\n",
        "        nc_df = df[[\"noise\",\"coupling\"]].reset_index(drop=True)\n",
        "    elif nc is not None and len(nc) == len(df):\n",
        "        nc_df = nc.reset_index(drop=True)\n",
        "    else:\n",
        "        # Fallback: create placeholders with NaNs but keep row index for traceability\n",
        "        nc_df = pd.DataFrame({\n",
        "            \"noise\": np.nan,\n",
        "            \"coupling\": np.nan,\n",
        "        }, index=range(len(df))).reset_index(drop=True)\n",
        "        if nc is not None:\n",
        "            print(f\"Warning: n_c_.csv length ({len(nc)}) != input rows ({len(df)}). Proceeding without attaching noise/coupling values.\")\n",
        "\n",
        "    # --- Feature matrix ---\n",
        "    feature_cols = [c for c in df.columns if c not in [\"noise\", \"coupling\", \"row_index\"]]\n",
        "    X = df[feature_cols].to_numpy()\n",
        "    print(\"Used features:\", feature_cols)\n",
        "\n",
        "    metrics = []\n",
        "    labels_dict = {}\n",
        "\n",
        "    for k in ks:\n",
        "        kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=n_init, algorithm=algorithm)\n",
        "        kmeans.fit(X)\n",
        "        labels = kmeans.labels_\n",
        "\n",
        "        sil = silhouette_score(X, labels)\n",
        "        db  = davies_bouldin_score(X, labels)\n",
        "\n",
        "        metrics.append({\n",
        "            \"k\": k,\n",
        "            \"silhouette\": sil,\n",
        "            \"davies_bouldin\": db,\n",
        "        })\n",
        "\n",
        "        labels_df = nc_df.copy()\n",
        "        labels_df[\"cluster\"] = labels\n",
        "        labels_df.to_csv(f\"clusters_{base}_k{k}.csv\", index=False)\n",
        "        labels_dict[k] = labels_df\n",
        "        globals()[f\"labels_k_{k}_df\"] = labels_df\n",
        "\n",
        "    metrics_df = pd.DataFrame(metrics, columns=[\"k\", \"silhouette\", \"davies_bouldin\"])\n",
        "    metrics_path = f\"metrics_{base}.csv\"\n",
        "    metrics_df.to_csv(metrics_path, index=False)\n",
        "\n",
        "    print(f\"Saved metrics -> {metrics_path}\")\n",
        "    print(f\"Saved labels  -> clusters_{base}_k{k_for_labels}.csv (and for all k in {ks})\")\n",
        "\n",
        "    return metrics_df, labels_dict[k_for_labels]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9bca3ad6-6ef3-4ede-a848-9e09c1ae63a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_labels_heatmap(labels_df, output_file=None, use_tex=True):\n",
        "\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from matplotlib.colors import ListedColormap\n",
        "\n",
        "    # --- column handling (case-insensitive)\n",
        "    col_map = {c.lower(): c for c in labels_df.columns}\n",
        "    for req in (\"noise\", \"coupling\", \"cluster\"):\n",
        "        if req not in col_map:\n",
        "            raise ValueError(f\"labels_df must contain '{req}' column.\")\n",
        "    ncol, ccol, lcol = col_map[\"noise\"], col_map[\"coupling\"], col_map[\"cluster\"]\n",
        "\n",
        "    # --- extract arrays\n",
        "    noise_vals = labels_df[ncol].astype(float).to_numpy()\n",
        "    coupling_vals = labels_df[ccol].astype(float).to_numpy()\n",
        "    cluster_vals = labels_df[lcol].astype(int).to_numpy()\n",
        "\n",
        "    # --- unique sorted axes\n",
        "    noise_unique = np.unique(noise_vals)\n",
        "    coupling_unique = np.unique(coupling_vals)\n",
        "\n",
        "    # --- grid and index maps (faster than np.where in a loop)\n",
        "    heatmap_data = np.full((len(coupling_unique), len(noise_unique)), np.nan, dtype=float)\n",
        "    noise_idx = {v: j for j, v in enumerate(noise_unique)}\n",
        "    coupling_idx = {v: i for i, v in enumerate(coupling_unique)}\n",
        "    for n, c, cl in zip(noise_vals, coupling_vals, cluster_vals):\n",
        "        j = noise_idx.get(n, None)\n",
        "        i = coupling_idx.get(c, None)\n",
        "        if i is not None and j is not None:\n",
        "            heatmap_data[i, j] = cl\n",
        "\n",
        "    # --- detect number of clusters (assumes 0-based labels)\n",
        "    k = int(np.nanmax(heatmap_data)) + 1\n",
        "\n",
        "    # --- discrete colormap with k colors\n",
        "    # tab20 handles up to 20 distinct colors; beyond that we request k colors from hsv\n",
        "    try:\n",
        "        cmap_base = plt.get_cmap(\"tab20\", k if k <= 20 else k)\n",
        "        #cmap_base = plt.cm.get_cmap(\"tab20\", k if k <= 20 else k)\n",
        "    except Exception:\n",
        "        cmap_base = plt.cm.get_cmap(\"hsv\", k)\n",
        "    cluster_cmap = ListedColormap(cmap_base.colors if hasattr(cmap_base, \"colors\")\n",
        "                                  else [cmap_base(i) for i in range(k)])\n",
        "\n",
        "    # --- styling\n",
        "    sns.set_theme(style=\"white\")\n",
        "    plt.rcParams.update({\n",
        "        \"text.usetex\": bool(use_tex),\n",
        "        \"font.family\": \"serif\",\n",
        "        \"axes.linewidth\": 1.2,\n",
        "        \"xtick.direction\": \"in\",\n",
        "        \"ytick.direction\": \"in\",\n",
        "        \"xtick.major.size\": 6,\n",
        "        \"ytick.major.size\": 6,\n",
        "        \"xtick.minor.size\": 3,\n",
        "        \"ytick.minor.size\": 3,\n",
        "    })\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    hm = sns.heatmap(\n",
        "        heatmap_data,\n",
        "        ax=ax,\n",
        "        xticklabels=np.round(noise_unique, 4),\n",
        "        yticklabels=np.round(coupling_unique, 4),\n",
        "        cmap=cluster_cmap,\n",
        "        cbar_kws={'label': 'Cluster', 'pad': 0.01, 'ticks': range(k)},\n",
        "        linewidths=0.1,\n",
        "        linecolor='gray',\n",
        "        vmin=0,\n",
        "        vmax=max(0, k - 1),\n",
        "    )\n",
        "\n",
        "    # colorbar cosmetics\n",
        "    cbar = hm.collections[0].colorbar\n",
        "    cbar.set_label('Cluster', rotation=90, labelpad=2, fontsize=16)\n",
        "    cbar.ax.yaxis.set_label_coords(2.5, 0.55)\n",
        "\n",
        "    # labels\n",
        "    ax.set_xlabel(r'$n$', fontsize=18)\n",
        "    ax.set_ylabel(r'$c$', fontsize=18, labelpad=10, rotation=0)\n",
        "    ax.tick_params(axis='x', labelsize=10, rotation=90)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if output_file:\n",
        "        plt.savefig(output_file, format=\"png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    return fig, ax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ff3ffb73-3ffa-497e-bd4f-ada38310efcc",
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 7) (2904521664.py, line 7)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m})[[\"Silhouette\", DB\"]]\u001b[39m\n                        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 7)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "input_file = \"set_B_pca_3_.csv\"\n",
        "metrics_df, labels_df = run_kmeans_on_file(input_file, ks=(3,4,5,6,7))\n",
        "\n",
        "scores = metrics_df.rename(columns={\n",
        "    \"silhouette\": \"Silhouette\",\n",
        "    \"davies_bouldin\": \"DB\"\n",
        "})[[\"Silhouette\", \"DB\"]]\n",
        "\n",
        "print(\"\\nScores per k (Silhouette higher, DB lower is better):\\n\")\n",
        "print(scores.to_string(float_format=lambda x: f\"{x:.6f}\"))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
